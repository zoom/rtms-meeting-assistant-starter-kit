// openrouterChat.js
import OpenAI from 'openai';
import dotenv from 'dotenv';
dotenv.config();

// Set up OpenAI client with OpenRouter endpoint
const openai = new OpenAI({
  apiKey: process.env.OPENROUTER_API_KEY,
  baseURL: 'https://openrouter.ai/api/v1',
});

/**
 * Sends a message to a model via OpenRouter
 * @param {string} message - The user message
 * @param {string} model - The model to use (e.g., 'anthropic/claude-3-haiku')
 * @returns {Promise<string>}
 */
export async function chatWithOpenRouter(message, model = process.env.OPENROUTER_MODEL || 'openai/gpt-5-chat') {


  try {
    const enabled = process.env.OPENROUTER_REASONING_ENABLED === 'true' || false;
    const response = await openai.chat.completions.create({
      model: model,
      messages: [{ role: 'user', content: message }],
      ...(enabled ? { reasoning: { enabled: true } } : {})
    });

    return response.choices[0].message.content;
  } catch (err) {
    console.error('‚ùå Error with OpenRouter:', err.response?.data || err.message);
    throw err;
  }
}
export async function chatWithOpenRouterFast(message, model = process.env.OPENROUTER_MODEL || 'openai/gpt-5-chat') {


  try {
    const enabled = process.env.OPENROUTER_REASONING_ENABLED === 'true' || false;
    const response = await openai.chat.completions.create({
      model: model,
      messages: [{ role: 'user', content: message }]
    });

    return response.choices[0].message.content;
  } catch (err) {
    console.error('‚ùå Error with OpenRouter:', err.response?.data || err.message);
    throw err;
  }
}

export async function chatWithMultipleModels(message) {
  const models = [
    'meta-llama/llama-4-maverick:free',
    'meta-llama/llama-4-scout:free',
  ];

  await Promise.all(models.map(async (model) => {
    try {
      const response = await openai.chat.completions.create({
        model,
        messages: [{ role: 'user', content: message }],
      });

      const reply = response.choices[0].message.content;

      console.log('='.repeat(60));
      console.log(`üß† MODEL: ${model}`);
      console.log('-'.repeat(60));
      console.log(`üí¨ RESPONSE:\n${reply}`);
      console.log('='.repeat(60));
    } catch (err) {
      console.error(`‚ùå Error with model ${model}:`, err.response?.data || err.message);
      console.log('='.repeat(60));
    }
  }));
}

export async function contextualSynthesisFromMultipleModels(message) {
  const models = [
    'meta-llama/llama-4-maverick:free',
    'meta-llama/llama-4-scout:free',
  ];

  console.log(`üì® Received prompt: "${message}"\n`);
  console.log(`ü§ñ Sending prompt to ${models.length} models in parallel...\n`);

  const modelTasks = models.map(async (model) => {
    try {
      console.log(`‚è≥ Querying model: ${model}`);
      const response = await openai.chat.completions.create({
        model,
        messages: [{ role: 'user', content: message }],
      });

      console.log(`‚úÖ Received response from ${model}`);
      return { model, reply: response.choices[0].message.content };
    } catch (err) {
      console.error(`‚ùå Error with model ${model}:`, err.response?.data || err.message);
      return null;
    }
  });

  const modelResponses = (await Promise.all(modelTasks)).filter(Boolean);

  if (modelResponses.length === 0) {
    console.error('‚ùå No successful responses to synthesize from.');
    return;
  }

  console.log('\nüß† All model responses received. Preparing for synthesis...\n');

  const combinedContext = modelResponses.map(({ model, reply }) =>
    `Response from ${model}:\n${reply}`
  ).join('\n\n');

  const synthesisPrompt = `
You are an expert assistant. The user asked:

"${message}"

Here are responses from multiple AI models. Cross-check the answers, validate facts, and generate a final answer that is accurate, clear, and well-supported. Do not summarize ‚Äî synthesize the best answer possible using their content.

${combinedContext}
  `.trim();

  const synthesisModel = 'anthropic/claude-3-haiku';

  try {
    console.log(`üß™ Synthesizing final answer using ${synthesisModel}...\n`);

    // Spinner: show elapsed seconds while waiting
    let seconds = 0;
    const spinner = setInterval(() => {
      seconds++;
      process.stdout.write(`‚è≥ Thinking... ${seconds}s\r`);
    }, 1000);

    const finalResponse = await openai.chat.completions.create({
      model: synthesisModel,
      messages: [{ role: 'user', content: synthesisPrompt }],
    });

    clearInterval(spinner);
    process.stdout.write('\n'); // move to clean line

    const finalAnswer = finalResponse.choices[0].message.content;

    console.log('\n‚úÖ FINAL SYNTHESIZED ANSWER');
    console.log('='.repeat(60));
    console.log(finalAnswer);
    console.log('='.repeat(60));
  } catch (err) {
    console.error(`‚ùå Error during synthesis:`, err.response?.data || err.message);
  }
}
